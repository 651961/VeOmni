model:
  model_path: /data/models/Wan2.2-I2V-A14B/high_noise_model
  config_path: ./configs/model_configs/wan/wan2_2_i2v_a14b.json
  attn_implementation: flash_attention_2
  lora_target_modules: q,k,v,o,ffn.0,ffn.2
  lora_target_modules_support: q,k,v,o,ffn.0,ffn.2
  lora_rank: 128
  lora_alpha: 128

data:
  train_path: /data/ckpt_zsqiao/codes/shuangren_dazhaohu_with_shouchiwu_dazhaohu_v4
  train_size: 1000000000000
  dataloader_type: native
  datasets_type: iterable
  data_type: diffusion
  text_keys: text
  drop_last: true
  datasets_repeat: 1

train:
  output_dir: /data/ckpt_zsqiao/sft_lora_high_noise_model_shuangren_dazhaohu_with_shouchiwu_dazhaohu_v4
  train_architecture: lora
  data_parallel_mode: fsdp1
  data_parallel_replicate_size: 1
  ulysses_parallel_size: 1
  global_batch_size: 16
  micro_batch_size: 1
  rmpad: false
  rmpad_with_pos_ids: false
  bsz_warmup_ratio: 0.007
  dyn_bsz_margin: 0
  dyn_bsz_buffer_size: 200
  seed: 42
  optimizer: adamw
  lr: 1.0e-4
  lr_warmup_ratio: 0.005
  lr_decay_style: constant
  lr_decay_ratio: 1.0
  weight_decay: 0.01
  max_grad_norm: 1.0
  enable_mixed_precision: true
  enable_gradient_checkpointing: true
  enable_full_shard: true
  enable_fsdp_offload: false
  enable_activation_offload: false
  init_device: cuda
  enable_full_determinism: false
  empty_cache_steps: 500
  ckpt_manager: dcp
  load_checkpoint_path: ""
  max_steps: 1000
  save_epochs: 40
  num_train_epochs: 200
  save_hf_weights: true
  use_wandb: false
  wandb_project: SFT-LoRA-Wan2.2-I2V-A14B
  wandb_name: sft_lora_wan2_2_i2v_a14b
  max_timestep_boundary: 0.358
  min_timestep_boundary: 0.0
  ops_to_save:
    # - aten.addmm.default
    - torch.ops.flash_attn._flash_attn_forward.default
    - aten._scaled_dot_product_flash_attention.default
    - torch.ops.flash_attn_3.fwd.default